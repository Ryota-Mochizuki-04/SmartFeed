"""
RSS LINE Notifier - RSSè§£æãƒ»è¨˜äº‹åˆ†æã‚¯ãƒ©ã‚¹
feedparserã‚’ä½¿ç”¨ã—ãŸRSSè§£æã¨ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆè¨˜äº‹åˆ†æã®å®Ÿè£…
"""

import re
import logging
import feedparser
import requests
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Optional, Tuple, Any
from urllib.parse import urljoin, urlparse
import concurrent.futures


# ãƒ­ã‚°è¨­å®š
logger = logging.getLogger(__name__)


class RSSAnalyzer:
    """RSSè§£æãƒ»è¨˜äº‹åˆ†æã‚¯ãƒ©ã‚¹"""

    # è¨˜äº‹ã‚¿ã‚¤ãƒ—åˆ†é¡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    ARTICLE_TYPE_KEYWORDS = {
        'ãƒˆãƒ¬ãƒ³ãƒ‰': ['äººæ°—', 'ãƒˆãƒ¬ãƒ³ãƒ‰', 'ãƒã‚º', 'è©±é¡Œ', 'ãƒ©ãƒ³ã‚­ãƒ³ã‚°', 'ã¾ã¨ã‚', 'ãƒˆãƒƒãƒ—'],
        'æŠ€è¡“è§£èª¬': ['è§£èª¬', 'å…¥é–€', 'ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«', 'ä½¿ã„æ–¹', 'æ–¹æ³•', 'ãƒã‚¦ãƒ„ãƒ¼', 'æ‰‹é †', 'åŸºç¤'],
        'ãƒ„ãƒ¼ãƒ«': ['ãƒ„ãƒ¼ãƒ«', 'ãƒ©ã‚¤ãƒ–ãƒ©ãƒª', 'ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯', 'ãƒ—ãƒ©ã‚°ã‚¤ãƒ³', 'ã‚¢ãƒ—ãƒª', 'ã‚½ãƒ•ãƒˆ'],
        'åˆ†æ': ['åˆ†æ', 'æ¤œè¨¼', 'æ¯”è¼ƒ', 'èª¿æŸ»', 'ãƒ¬ãƒãƒ¼ãƒˆ', 'çµ±è¨ˆ', 'ãƒ‡ãƒ¼ã‚¿'],
        'ãƒ‹ãƒ¥ãƒ¼ã‚¹': ['ãƒªãƒªãƒ¼ã‚¹', 'ç™ºè¡¨', 'ç™ºå£²', 'ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ', 'æ›´æ–°', 'ãƒãƒ¼ã‚¸ãƒ§ãƒ³', 'ãƒ‹ãƒ¥ãƒ¼ã‚¹']
    }

    # é›£æ˜“åº¦åˆ¤å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    DIFFICULTY_KEYWORDS = {
        'åˆç´š': ['åˆå¿ƒè€…', 'å…¥é–€', 'åŸºç¤', 'åŸºæœ¬', 'ã¯ã˜ã‚ã¦', 'ç°¡å˜', 'ã‚„ã•ã—ã„'],
        'ä¸Šç´š': ['ä¸Šç´š', 'é«˜åº¦', 'ã‚¢ãƒ‰ãƒãƒ³ã‚¹', 'è©³ç´°', 'æ·±ã„', 'ãƒ—ãƒ­', 'ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ'],
        # ä¸­ç´šã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    }

    def __init__(self, request_timeout: int = 30, max_workers: int = 10):
        """
        RSSè§£æã‚¯ãƒ©ã‚¹åˆæœŸåŒ–

        Args:
            request_timeout: HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰
            max_workers: ä¸¦åˆ—å‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
        """
        self.request_timeout = request_timeout
        self.max_workers = max_workers
        self.logger = logging.getLogger(__name__)

        # HTTP ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'RSS-LINE-Notifier/2.1 (+Python feedparser)'
        })

    def fetch_and_analyze_feeds(self, feeds: List[Dict]) -> Dict[str, List[Dict]]:
        """
        è¤‡æ•°RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ä¸¦åˆ—å–å¾—ãƒ»åˆ†æ

        Args:
            feeds: RSSãƒ•ã‚£ãƒ¼ãƒ‰è¨­å®šãƒªã‚¹ãƒˆ

        Returns:
            Dict: ã‚«ãƒ†ã‚´ãƒªåˆ¥è¨˜äº‹è¾æ›¸
        """
        try:
            # æœ‰åŠ¹ãªãƒ•ã‚£ãƒ¼ãƒ‰ã®ã¿å‡¦ç†
            active_feeds = [feed for feed in feeds if feed.get('enabled', True)]

            if not active_feeds:
                self.logger.warning("æœ‰åŠ¹ãªRSSãƒ•ã‚£ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")
                return {}

            # ä¸¦åˆ—å‡¦ç†ã§RSSãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—
            all_articles = []
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_feed = {
                    executor.submit(self._fetch_single_feed, feed): feed
                    for feed in active_feeds
                }

                for future in concurrent.futures.as_completed(future_to_feed):
                    feed = future_to_feed[future]
                    try:
                        articles = future.result()
                        if articles:
                            all_articles.extend(articles)
                            self.logger.info(f"ãƒ•ã‚£ãƒ¼ãƒ‰ '{feed['title']}' ã‹ã‚‰ {len(articles)} ä»¶ã®è¨˜äº‹ã‚’å–å¾—")
                    except Exception as e:
                        self.logger.error(f"ãƒ•ã‚£ãƒ¼ãƒ‰ '{feed['title']}' ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")

            if not all_articles:
                self.logger.warning("å–å¾—ã§ããŸè¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“")
                return {}

            # æ–°ç€è¨˜äº‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            recent_articles = self._filter_recent_articles(all_articles)
            self.logger.info(f"æ–°ç€è¨˜äº‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: {len(recent_articles)} / {len(all_articles)} ä»¶")

            # è¨˜äº‹åˆ†æãƒ»ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
            analyzed_articles = self._analyze_articles(recent_articles)

            # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            categorized = self._group_by_category(analyzed_articles)

            # ã‚«ãƒ†ã‚´ãƒªå†…å„ªå…ˆé †ä½è¨­å®š
            for category, articles in categorized.items():
                categorized[category] = self._rank_articles(articles)

            total_articles = sum(len(articles) for articles in categorized.values())
            self.logger.info(f"è¨˜äº‹åˆ†æå®Œäº†: {total_articles} ä»¶ã‚’ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«åˆ†é¡")

            return categorized

        except Exception as e:
            self.logger.error(f"RSSå–å¾—ãƒ»åˆ†æå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    def _fetch_single_feed(self, feed: Dict) -> List[Dict]:
        """å˜ä¸€RSSãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—"""
        try:
            url = feed['url']
            feed_id = feed['id']
            feed_title = feed['title']
            feed_category = feed['category']

            self.logger.info(f"RSSãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—é–‹å§‹: {feed_title}")

            # feedparser ã§RSSè§£æ
            parsed_feed = feedparser.parse(url)

            if parsed_feed.bozo and parsed_feed.bozo_exception:
                self.logger.warning(f"RSSè§£æè­¦å‘Š '{feed_title}': {parsed_feed.bozo_exception}")

            if not hasattr(parsed_feed, 'entries') or not parsed_feed.entries:
                self.logger.warning(f"ãƒ•ã‚£ãƒ¼ãƒ‰ '{feed_title}' ã«è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“")
                return []

            articles = []
            for entry in parsed_feed.entries:
                try:
                    article = self._parse_entry(entry, feed_id, feed_title, feed_category)
                    if article:
                        articles.append(article)
                except Exception as e:
                    self.logger.warning(f"è¨˜äº‹è§£æã§ã‚¨ãƒ©ãƒ¼: {e}")
                    continue

            self.logger.info(f"ãƒ•ã‚£ãƒ¼ãƒ‰ '{feed_title}' ã‹ã‚‰ {len(articles)} ä»¶ã®è¨˜äº‹ã‚’è§£æ")
            return articles

        except Exception as e:
            self.logger.error(f"ãƒ•ã‚£ãƒ¼ãƒ‰ '{feed['title']}' ã®å–å¾—ã«å¤±æ•—: {e}")
            return []

    def _parse_entry(self, entry: Any, feed_id: str, feed_title: str, feed_category: str) -> Optional[Dict]:
        """RSS ã‚¨ãƒ³ãƒˆãƒªè§£æ"""
        try:
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ç¢ºèª
            title = getattr(entry, 'title', '').strip()
            link = getattr(entry, 'link', '').strip()

            if not title or not link:
                return None

            # å…¬é–‹æ—¥æ™‚å‡¦ç†
            published_at = self._parse_published_date(entry)

            # æ¦‚è¦ãƒ»å†…å®¹å–å¾—
            description = self._extract_description(entry)

            # ç”»åƒURLå–å¾—
            image_url = self._extract_image_url(entry)

            article = {
                'title': title,
                'link': link,
                'description': description,
                'published_at': published_at,
                'feed_id': feed_id,
                'feed_title': feed_title,
                'category': feed_category,
                'image_url': image_url,
                'raw_entry': {
                    'author': getattr(entry, 'author', ''),
                    'tags': [tag.term for tag in getattr(entry, 'tags', [])],
                    'id': getattr(entry, 'id', '')
                }
            }

            return article

        except Exception as e:
            self.logger.warning(f"ã‚¨ãƒ³ãƒˆãƒªè§£æã§ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def _parse_published_date(self, entry: Any) -> str:
        """å…¬é–‹æ—¥æ™‚è§£æ"""
        try:
            # è¤‡æ•°ã®æ—¥æ™‚ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è©¦è¡Œ
            date_fields = ['published_parsed', 'updated_parsed']

            for field in date_fields:
                date_tuple = getattr(entry, field, None)
                if date_tuple:
                    dt = datetime(*date_tuple[:6], tzinfo=timezone.utc)
                    return dt.isoformat()

            # æ–‡å­—åˆ—å½¢å¼ã®æ—¥æ™‚ã‚’è©¦è¡Œ
            string_fields = ['published', 'updated']
            for field in string_fields:
                date_string = getattr(entry, field, '')
                if date_string:
                    # ç°¡æ˜“çš„ãªæ—¥æ™‚è§£æï¼ˆfeedparserãŒè§£ææ¸ˆã¿ã®å ´åˆï¼‰
                    return date_string

            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ç¾åœ¨æ™‚åˆ»
            return datetime.now(timezone.utc).isoformat()

        except Exception:
            return datetime.now(timezone.utc).isoformat()

    def _extract_description(self, entry: Any) -> str:
        """è¨˜äº‹æ¦‚è¦æŠ½å‡º"""
        try:
            # è¤‡æ•°ã®æ¦‚è¦ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è©¦è¡Œ
            desc_fields = ['summary', 'description', 'content']

            for field in desc_fields:
                content = getattr(entry, field, '')
                if isinstance(content, list) and content:
                    content = content[0].get('value', '') if isinstance(content[0], dict) else str(content[0])

                if content:
                    # HTMLã‚¿ã‚°é™¤å»
                    clean_desc = re.sub(r'<[^>]+>', '', str(content))
                    # ä½™åˆ†ãªç©ºç™½ãƒ»æ”¹è¡Œé™¤å»
                    clean_desc = re.sub(r'\s+', ' ', clean_desc).strip()
                    # æ–‡å­—æ•°åˆ¶é™
                    if len(clean_desc) > 200:
                        clean_desc = clean_desc[:200] + '...'
                    return clean_desc

            return ''

        except Exception:
            return ''

    def _extract_image_url(self, entry: Any) -> Optional[str]:
        """è¨˜äº‹ç”»åƒURLæŠ½å‡º"""
        try:
            # enclosuresï¼ˆæ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã‹ã‚‰ç”»åƒæ¤œç´¢
            enclosures = getattr(entry, 'enclosures', [])
            for enclosure in enclosures:
                if hasattr(enclosure, 'type') and enclosure.type.startswith('image/'):
                    return enclosure.href

            # media_thumbnailï¼ˆãƒ¡ãƒ‡ã‚£ã‚¢ã‚µãƒ ãƒã‚¤ãƒ«ï¼‰
            if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
                return entry.media_thumbnail[0]['url']

            # contentå†…ã®ç”»åƒæ¤œç´¢
            content_fields = ['content', 'summary']
            for field in content_fields:
                content = getattr(entry, field, '')
                if isinstance(content, list) and content:
                    content = content[0].get('value', '') if isinstance(content[0], dict) else str(content[0])

                if content:
                    # img ã‚¿ã‚°ã‹ã‚‰ src å±æ€§æŠ½å‡º
                    img_match = re.search(r'<img[^>]+src=["\']([^"\']+)["\']', str(content))
                    if img_match:
                        return img_match.group(1)

            return None

        except Exception:
            return None

    def _filter_recent_articles(self, articles: List[Dict], hours: int = 24) -> List[Dict]:
        """æ–°ç€è¨˜äº‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        try:
            cutoff_time = datetime.now(timezone.utc) - timedelta(hours=hours)
            recent_articles = []

            for article in articles:
                try:
                    published_str = article.get('published_at', '')
                    if published_str:
                        # ISOå½¢å¼ã®æ—¥æ™‚ã‚’ãƒ‘ãƒ¼ã‚¹
                        published_dt = datetime.fromisoformat(published_str.replace('Z', '+00:00'))
                        if published_dt >= cutoff_time:
                            recent_articles.append(article)
                    else:
                        # å…¬é–‹æ—¥æ™‚ä¸æ˜ã®å ´åˆã¯å«ã‚ã‚‹
                        recent_articles.append(article)
                except Exception as e:
                    self.logger.warning(f"æ—¥æ™‚ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã§ã‚¨ãƒ©ãƒ¼: {e}")
                    # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å«ã‚ã‚‹
                    recent_articles.append(article)

            return recent_articles

        except Exception as e:
            self.logger.error(f"æ–°ç€è¨˜äº‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã§ã‚¨ãƒ©ãƒ¼: {e}")
            return articles

    def _analyze_articles(self, articles: List[Dict]) -> List[Dict]:
        """è¨˜äº‹åˆ†æï¼ˆv2.1 ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆåˆ†æï¼‰"""
        try:
            analyzed_articles = []

            for article in articles:
                # è¨˜äº‹ã‚¿ã‚¤ãƒ—åˆ†é¡
                article_type = self._classify_article_type(article)

                # é›£æ˜“åº¦æ¨å®š
                difficulty = self._estimate_difficulty(article)

                # èª­äº†æ™‚é–“æ¨å®š
                reading_time = self._estimate_reading_time(article)

                # å„ªå…ˆé †ä½ã‚¹ã‚³ã‚¢è¨ˆç®—
                priority_score = self._calculate_priority_score(article, article_type, difficulty)

                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
                article['metadata'] = {
                    'article_type': article_type,
                    'difficulty': difficulty,
                    'reading_time': reading_time,
                    'priority_score': priority_score,
                    'analyzed_at': datetime.now(timezone.utc).isoformat()
                }

                analyzed_articles.append(article)

            return analyzed_articles

        except Exception as e:
            self.logger.error(f"è¨˜äº‹åˆ†æã§ã‚¨ãƒ©ãƒ¼: {e}")
            return articles

    def _classify_article_type(self, article: Dict) -> str:
        """
        è¨˜äº‹ã‚¿ã‚¤ãƒ—è‡ªå‹•åˆ†é¡ï¼ˆv2.1å¼·åŒ–ç‰ˆï¼‰
        rss-line-notifierã®é«˜åº¦ãªåˆ†é¡ãƒ­ã‚¸ãƒƒã‚¯ã‚’çµ±åˆ

        Returns:
            str: è¨˜äº‹ã‚¿ã‚¤ãƒ—ã‚¢ã‚¤ã‚³ãƒ³ï¼ˆğŸ”¥ãƒˆãƒ¬ãƒ³ãƒ‰, âš¡æŠ€è¡“è§£èª¬, ğŸ› ï¸ãƒ„ãƒ¼ãƒ«, ğŸ“Šåˆ†æ, ğŸ“°ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼‰
        """
        try:
            title = article.get('title', '')
            description = article.get('description', '')
            feed_title = article.get('feed_title', '')

            title_lower = title.lower()
            desc_lower = description.lower()
            feed_lower = feed_title.lower()

            # ãƒˆãƒ¬ãƒ³ãƒ‰ç³»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆrss-line-notifierã‹ã‚‰ç§»æ¤ï¼‰
            trending_keywords = ["è©±é¡Œ", "äººæ°—", "æ³¨ç›®", "ãƒã‚º", "è©±é¡Œæ²¸é¨°", "æ€¥ä¸Šæ˜‡", "ãƒ©ãƒ³ã‚­ãƒ³ã‚°"]
            if any(keyword in title for keyword in trending_keywords) or "popular" in title_lower or "trend" in title_lower:
                return "ğŸ”¥"

            # æŠ€è¡“è§£èª¬ç³»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            technical_keywords = ["è§£èª¬", "å…¥é–€", "åŸºç¤", "åˆå¿ƒè€…", "å­¦ç¿’", "ç†è§£", "ä»•çµ„ã¿", "åŸç†"]
            if any(keyword in title for keyword in technical_keywords) or "tutorial" in title_lower or "guide" in title_lower:
                return "âš¡"

            # ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç³»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            tool_keywords = ["ãƒ„ãƒ¼ãƒ«", "ãƒ©ã‚¤ãƒ–ãƒ©ãƒª", "ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯", "ã‚¢ãƒ—ãƒª", "ã‚µãƒ¼ãƒ“ã‚¹", "ä½¿ã„æ–¹", "å°å…¥"]
            if any(keyword in title for keyword in tool_keywords) or "tool" in title_lower or "library" in title_lower:
                return "ğŸ› ï¸"

            # åˆ†æãƒ»èª¿æŸ»ç³»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            analysis_keywords = ["åˆ†æ", "èª¿æŸ»", "ãƒ¬ãƒãƒ¼ãƒˆ", "çµ±è¨ˆ", "ãƒ‡ãƒ¼ã‚¿", "æ¯”è¼ƒ", "æ¤œè¨¼", "è€ƒå¯Ÿ"]
            if any(keyword in title for keyword in analysis_keywords) or "analysis" in title_lower or "report" in title_lower:
                return "ğŸ“Š"

            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ãƒ‹ãƒ¥ãƒ¼ã‚¹
            return "ğŸ“°"

        except Exception:
            return "ğŸ“°"

    def _estimate_difficulty(self, article: Dict) -> str:
        """é›£æ˜“åº¦æ¨å®š"""
        try:
            text = f"{article['title']} {article.get('description', '')}"
            text_lower = text.lower()

            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
            for difficulty, keywords in self.DIFFICULTY_KEYWORDS.items():
                for keyword in keywords:
                    if keyword in text_lower:
                        return difficulty

            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ä¸­ç´š
            return 'ä¸­ç´š'

        except Exception:
            return 'ä¸­ç´š'

    def _estimate_reading_time(self, article: Dict) -> str:
        """èª­äº†æ™‚é–“æ¨å®š"""
        try:
            text = article.get('description', '')
            # æ–‡å­—æ•°ãƒ™ãƒ¼ã‚¹ã®ç°¡æ˜“æ¨å®šï¼ˆæ—¥æœ¬èª: 600æ–‡å­—/åˆ†ï¼‰
            char_count = len(text)

            if char_count < 300:
                return '1åˆ†'
            elif char_count < 600:
                return '2åˆ†'
            elif char_count < 1200:
                return '3åˆ†'
            elif char_count < 1800:
                return '5åˆ†'
            else:
                minutes = max(5, char_count // 300)
                return f'{minutes}åˆ†'

        except Exception:
            return '3åˆ†'

    def _calculate_priority_score(self, article: Dict, article_type: str, difficulty: str) -> float:
        """
        å„ªå…ˆé †ä½ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆv2.1å¼·åŒ–ç‰ˆï¼‰
        rss-line-notifierã®é«˜åº¦ãªã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’çµ±åˆ
        """
        try:
            score = 50.0  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢
            title = article.get('title', '')

            # è¨˜äº‹ã‚¿ã‚¤ãƒ—ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘ï¼ˆã‚¢ã‚¤ã‚³ãƒ³ãƒ™ãƒ¼ã‚¹ï¼‰
            type_weights = {
                'ğŸ”¥': 30,  # ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆæœ€é«˜å„ªå…ˆåº¦ï¼‰
                'ğŸ“°': 25,  # ãƒ‹ãƒ¥ãƒ¼ã‚¹
                'âš¡': 20,  # æŠ€è¡“è§£èª¬
                'ğŸ› ï¸': 15,  # ãƒ„ãƒ¼ãƒ«
                'ğŸ“Š': 10,  # åˆ†æ
            }
            score += type_weights.get(article_type, 5)

            # é›£æ˜“åº¦ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘
            difficulty_weights = {
                'åˆç´š': 15,  # åˆå¿ƒè€…å‘ã‘ã¯å„ªå…ˆåº¦é«˜
                'ä¸­ç´š': 10,
                'ä¸Šç´š': 5
            }
            score += difficulty_weights.get(difficulty, 0)

            # rss-line-notifierã‹ã‚‰ç§»æ¤ï¼šãƒˆãƒ¬ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ã‚¹ã‚³ã‚¢åŠ ç®—
            if any(keyword in title for keyword in ["äººæ°—", "è©±é¡Œ", "æ³¨ç›®"]):
                score += 10

            # å®Ÿè·µçš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ã‚¹ã‚³ã‚¢åŠ ç®—
            if any(keyword in title for keyword in ["å®Ÿè·µ", "å®Ÿè£…", "ä½œã‚Šæ–¹", "ã‚„ã‚Šæ–¹"]):
                score += 5

            # è¨˜äº‹ã®æ–°ã—ã•ï¼ˆ24æ™‚é–“ä»¥å†…ã®è¨˜äº‹ã¯é«˜ã‚¹ã‚³ã‚¢ï¼‰
            try:
                published_str = article.get('published_at', '')
                if published_str:
                    published_dt = datetime.fromisoformat(published_str.replace('Z', '+00:00'))
                    hours_ago = (datetime.now(timezone.utc) - published_dt).total_seconds() / 3600
                    if hours_ago <= 6:
                        score += 20  # 6æ™‚é–“ä»¥å†…ã¯æœ€å„ªå…ˆ
                    elif hours_ago <= 12:
                        score += 15  # 12æ™‚é–“ä»¥å†…
                    elif hours_ago <= 24:
                        score += 10  # 24æ™‚é–“ä»¥å†…
            except:
                pass

            # ã‚¿ã‚¤ãƒˆãƒ«ã®é•·ã•ï¼ˆé©åº¦ãªé•·ã•ãŒå¥½ã¾ã—ã„ï¼‰
            title_len = len(title)
            if 20 <= title_len <= 60:
                score += 5
            elif title_len > 80:
                score -= 5  # é•·ã™ãã‚‹ã‚¿ã‚¤ãƒˆãƒ«ã¯æ¸›ç‚¹

            return min(100.0, max(0.0, score))

        except Exception:
            return 50.0

    def _group_by_category(self, articles: List[Dict]) -> Dict[str, List[Dict]]:
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚°ãƒ«ãƒ¼ãƒ—åŒ–"""
        try:
            categorized = {}

            for article in articles:
                category = article.get('category', 'ãã®ä»–')
                if category not in categorized:
                    categorized[category] = []
                categorized[category].append(article)

            return categorized

        except Exception as e:
            self.logger.error(f"ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã§ã‚¨ãƒ©ãƒ¼: {e}")
            return {'ãã®ä»–': articles}

    def _rank_articles(self, articles: List[Dict]) -> List[Dict]:
        """è¨˜äº‹ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¨­å®š"""
        try:
            # å„ªå…ˆé †ä½ã‚¹ã‚³ã‚¢ã§é™é †ã‚½ãƒ¼ãƒˆ
            sorted_articles = sorted(
                articles,
                key=lambda x: x.get('metadata', {}).get('priority_score', 0),
                reverse=True
            )

            # ãƒ©ãƒ³ã‚­ãƒ³ã‚°æƒ…å ±è¿½åŠ 
            for i, article in enumerate(sorted_articles):
                if 'metadata' not in article:
                    article['metadata'] = {}
                article['metadata']['rank'] = i + 1

            return sorted_articles

        except Exception as e:
            self.logger.error(f"è¨˜äº‹ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¨­å®šã§ã‚¨ãƒ©ãƒ¼: {e}")
            return articles